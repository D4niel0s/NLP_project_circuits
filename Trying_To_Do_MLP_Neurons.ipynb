{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook - intended for development only!\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danik\\AppData\\Local\\Temp\\ipykernel_14596\\3902491722.py:18: DeprecationWarning:\n",
      "\n",
      "`magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "\n",
      "C:\\Users\\Danik\\AppData\\Local\\Temp\\ipykernel_14596\\3902491722.py:19: DeprecationWarning:\n",
      "\n",
      "`magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
    "import os\n",
    "\n",
    "DEBUG_MODE = False\n",
    "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "    ipython.magic(\"load_ext autoreload\")\n",
    "    ipython.magic(\"autoreload 2\")\n",
    "\n",
    "if IN_COLAB or IN_GITHUB:\n",
    "    %pip install transformer_lens\n",
    "    %pip install torchtyping\n",
    "    # Install my janky personal plotting utils\n",
    "    %pip install git+https://github.com/neelnanda-io/neel-plotly.git\n",
    "    # Install another version of node that makes PySvelte work way faster\n",
    "    %pip install circuitsvis\n",
    "    # Needed for PySvelte to work, v3 came out and broke things...\n",
    "    %pip install typeguard==2.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "\n",
    "if IN_COLAB or not DEBUG_MODE:\n",
    "    # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"notebook_connected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "from typing import List, Union, Optional, Callable\n",
    "from functools import partial\n",
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import (\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neel_plotly import line, imshow, scatter\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'vscode'\n",
    "\n",
    "import transformer_lens.patching as patching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOI patching setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "\n",
    "\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_attn_in(True)\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean string 0 <|endoftext|>When John and Mary went to the shops, John gave the bag to\n",
      "Corrupted string 0 <|endoftext|>When John and Mary went to the shops, Mary gave the bag to\n",
      "Answer token indices tensor([[ 5335,  1757],\n",
      "        [ 1757,  5335],\n",
      "        [ 4186,  3700],\n",
      "        [ 3700,  4186],\n",
      "        [ 6035, 15686],\n",
      "        [15686,  6035],\n",
      "        [ 5780, 14235],\n",
      "        [14235,  5780]])\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"When John and Mary went to the shops, John gave the bag to\",\n",
    "    \"When John and Mary went to the shops, Mary gave the bag to\",\n",
    "    \"When Tom and James went to the park, James gave the ball to\",\n",
    "    \"When Tom and James went to the park, Tom gave the ball to\",\n",
    "    \"When Dan and Sid went to the shops, Sid gave an apple to\",\n",
    "    \"When Dan and Sid went to the shops, Dan gave an apple to\",\n",
    "    \"After Martin and Amy went to the park, Amy gave a drink to\",\n",
    "    \"After Martin and Amy went to the park, Martin gave a drink to\",\n",
    "]\n",
    "answers = [\n",
    "    (\" Mary\", \" John\"),\n",
    "    (\" John\", \" Mary\"),\n",
    "    (\" Tom\", \" James\"),\n",
    "    (\" James\", \" Tom\"),\n",
    "    (\" Dan\", \" Sid\"),\n",
    "    (\" Sid\", \" Dan\"),\n",
    "    (\" Martin\", \" Amy\"),\n",
    "    (\" Amy\", \" Martin\"),\n",
    "]\n",
    "\n",
    "clean_tokens = model.to_tokens(prompts)\n",
    "# Swap each adjacent pair, with a hacky list comprehension\n",
    "corrupted_tokens = clean_tokens[\n",
    "    [(i + 1 if i % 2 == 0 else i - 1) for i in range(len(clean_tokens))]\n",
    "]\n",
    "print(\"Clean string 0\", model.to_string(clean_tokens[0]))\n",
    "print(\"Corrupted string 0\", model.to_string(corrupted_tokens[0]))\n",
    "\n",
    "answer_token_indices = torch.tensor(\n",
    "    [\n",
    "        [model.to_single_token(answers[i][j]) for j in range(2)]\n",
    "        for i in range(len(answers))\n",
    "    ],\n",
    "    device=model.cfg.device,\n",
    ")\n",
    "print(\"Answer token indices\", answer_token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit diff: 3.5519\n",
      "Corrupted logit diff: -3.5519\n"
     ]
    }
   ],
   "source": [
    "def get_logit_diff(logits, answer_token_indices=answer_token_indices):\n",
    "    if len(logits.shape) == 3:\n",
    "        # Get final logits only\n",
    "        logits = logits[:, -1, :]\n",
    "    correct_logits = logits.gather(1, answer_token_indices[:, 0].unsqueeze(1))\n",
    "    incorrect_logits = logits.gather(1, answer_token_indices[:, 1].unsqueeze(1))\n",
    "    return (correct_logits - incorrect_logits).mean()\n",
    "\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "\n",
    "clean_logit_diff = get_logit_diff(clean_logits, answer_token_indices).item()\n",
    "print(f\"Clean logit diff: {clean_logit_diff:.4f}\")\n",
    "\n",
    "corrupted_logit_diff = get_logit_diff(corrupted_logits, answer_token_indices).item()\n",
    "print(f\"Corrupted logit diff: {corrupted_logit_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Baseline is 1: 1.0000\n",
      "Corrupted Baseline is 0: 0.0000\n"
     ]
    }
   ],
   "source": [
    "CLEAN_BASELINE = clean_logit_diff\n",
    "CORRUPTED_BASELINE = corrupted_logit_diff\n",
    "\n",
    "\n",
    "def ioi_metric(logits, answer_token_indices=answer_token_indices):\n",
    "    return (get_logit_diff(logits, answer_token_indices) - CORRUPTED_BASELINE) / (\n",
    "        CLEAN_BASELINE - CORRUPTED_BASELINE\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"Clean Baseline is 1: {ioi_metric(clean_logits).item():.4f}\")\n",
    "print(f\"Corrupted Baseline is 0: {ioi_metric(corrupted_logits).item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metric = Callable[[TT[\"batch_and_pos_dims\", \"d_model\"]], float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.mlp.hook_pre\n",
      "blocks.0.mlp.hook_post\n",
      "blocks.0.hook_mlp_in\n",
      "blocks.0.hook_mlp_out\n",
      "blocks.1.mlp.hook_pre\n",
      "blocks.1.mlp.hook_post\n",
      "blocks.1.hook_mlp_in\n",
      "blocks.1.hook_mlp_out\n",
      "blocks.2.mlp.hook_pre\n",
      "blocks.2.mlp.hook_post\n",
      "blocks.2.hook_mlp_in\n",
      "blocks.2.hook_mlp_out\n",
      "blocks.3.mlp.hook_pre\n",
      "blocks.3.mlp.hook_post\n",
      "blocks.3.hook_mlp_in\n",
      "blocks.3.hook_mlp_out\n",
      "blocks.4.mlp.hook_pre\n",
      "blocks.4.mlp.hook_post\n",
      "blocks.4.hook_mlp_in\n",
      "blocks.4.hook_mlp_out\n",
      "blocks.5.mlp.hook_pre\n",
      "blocks.5.mlp.hook_post\n",
      "blocks.5.hook_mlp_in\n",
      "blocks.5.hook_mlp_out\n",
      "blocks.6.mlp.hook_pre\n",
      "blocks.6.mlp.hook_post\n",
      "blocks.6.hook_mlp_in\n",
      "blocks.6.hook_mlp_out\n",
      "blocks.7.mlp.hook_pre\n",
      "blocks.7.mlp.hook_post\n",
      "blocks.7.hook_mlp_in\n",
      "blocks.7.hook_mlp_out\n",
      "blocks.8.mlp.hook_pre\n",
      "blocks.8.mlp.hook_post\n",
      "blocks.8.hook_mlp_in\n",
      "blocks.8.hook_mlp_out\n",
      "blocks.9.mlp.hook_pre\n",
      "blocks.9.mlp.hook_post\n",
      "blocks.9.hook_mlp_in\n",
      "blocks.9.hook_mlp_out\n",
      "blocks.10.mlp.hook_pre\n",
      "blocks.10.mlp.hook_post\n",
      "blocks.10.hook_mlp_in\n",
      "blocks.10.hook_mlp_out\n",
      "blocks.11.mlp.hook_pre\n",
      "blocks.11.mlp.hook_post\n",
      "blocks.11.hook_mlp_in\n",
      "blocks.11.hook_mlp_out\n"
     ]
    }
   ],
   "source": [
    "for h in model.hook_dict:\n",
    "    if \"mlp\" in h:\n",
    "        print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Value: 1.0\n",
      "Clean Activations Cached: 48\n",
      "Clean Gradients Cached: 48\n",
      "Corrupted Value: 0.0\n",
      "Corrupted Activations Cached: 48\n",
      "Corrupted Gradients Cached: 48\n"
     ]
    }
   ],
   "source": [
    "def filter_mlp_hooks(name):\n",
    "    if \"hook_mlp_in\" in name or \"hook_mlp_out\" in name:\n",
    "        return True\n",
    "    if \"mlp.hook_pre\" in name or \"mlp.hook_post\" in name:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def get_cache_fwd_and_bwd(model, tokens, metric):\n",
    "    model.reset_hooks()\n",
    "    cache = {}\n",
    "\n",
    "    def forward_cache_hook(act, hook):\n",
    "        cache[hook.name] = act.detach()\n",
    "\n",
    "    model.add_hook(filter_mlp_hooks, forward_cache_hook, \"fwd\")\n",
    "\n",
    "    grad_cache = {}\n",
    "\n",
    "    def backward_cache_hook(act, hook):\n",
    "        grad_cache[hook.name] = act.detach()\n",
    "\n",
    "    model.add_hook(filter_mlp_hooks, backward_cache_hook, \"bwd\")\n",
    "\n",
    "    value = metric(model(tokens))\n",
    "    value.backward()\n",
    "    model.reset_hooks()\n",
    "    return (\n",
    "        value.item(),\n",
    "        ActivationCache(cache, model),\n",
    "        ActivationCache(grad_cache, model),\n",
    "    )\n",
    "\n",
    "\n",
    "clean_value, clean_cache, clean_grad_cache = get_cache_fwd_and_bwd(\n",
    "    model, clean_tokens, ioi_metric\n",
    ")\n",
    "print(\"Clean Value:\", clean_value)\n",
    "print(\"Clean Activations Cached:\", len(clean_cache))\n",
    "print(\"Clean Gradients Cached:\", len(clean_grad_cache))\n",
    "corrupted_value, corrupted_cache, corrupted_grad_cache = get_cache_fwd_and_bwd(\n",
    "    model, corrupted_tokens, ioi_metric\n",
    ")\n",
    "print(\"Corrupted Value:\", corrupted_value)\n",
    "print(\"Corrupted Activations Cached:\", len(corrupted_cache))\n",
    "print(\"Corrupted Gradients Cached:\", len(corrupted_grad_cache))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 15, 3072])\n",
      "torch.Size([8, 15, 3072])\n",
      "torch.Size([8, 15, 768])\n",
      "torch.Size([8, 15, 768])\n"
     ]
    }
   ],
   "source": [
    "print(clean_cache['blocks.0.mlp.hook_pre'].shape)\n",
    "print(clean_cache['blocks.0.mlp.hook_post'].shape)\n",
    "print(clean_cache['blocks.0.hook_mlp_in'].shape)\n",
    "print(clean_cache['blocks.0.hook_mlp_out'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution patching"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
